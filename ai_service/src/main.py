from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel
import requests, os
from untils.text_extract import (
    extract_text_from_pdf_bytes,
    extract_text_from_docx,
)
from vector_store import query_law

app = FastAPI()

# âœ… Cáº¥u hÃ¬nh LM Studio + Phi-3 Mini
LM_STUDIO_API = "http://localhost:1234/v1/chat/completions"
MODEL_NAME = "microsoft/phi-3-mini-4k-instruct"


@app.get("/")
def read_root():
    return {"message": "AI Legal Contract & Law Chatbot is running ğŸš€"}


# ğŸ“„ PHÃ‚N TÃCH Há»¢P Äá»’NG
@app.post("/analyze_contract")
async def analyze_contract(file: UploadFile = File(...)):
    os.makedirs("uploads", exist_ok=True)
    file_path = os.path.join("uploads", file.filename)

    with open(file_path, "wb") as f:
        f.write(await file.read())

    if file.filename.endswith(".pdf"):
       with open(file_path, "rb") as f:
         file_bytes = f.read()
       contract_text = extract_text_from_pdf_bytes(file_bytes)
    elif file.filename.endswith(".docx"):
        contract_text = extract_text_from_docx(file_path)
    else:
        return {"error": "Chá»‰ há»— trá»£ file PDF hoáº·c DOCX."}

    # ğŸ” Láº¥y Ä‘iá»u luáº­t liÃªn quan
    related_laws = query_law(contract_text[:300])
    law_context = "\n\n".join(
        [f"Äiá»u {l['article_number']}: {l['article_title']}\n{l['content']}" for l in related_laws]
    )

    prompt = f"""
Báº¡n lÃ  chuyÃªn gia phÃ¡p lÃ½ Viá»‡t Nam.

--- Ná»™i dung há»£p Ä‘á»“ng ---
{contract_text[:1000]}

--- Äiá»u luáº­t liÃªn quan ---
{law_context}

HÃ£y:
1. TÃ³m táº¯t há»£p Ä‘á»“ng.
2. ÄÃ¡nh giÃ¡ quyá»n lá»£i & nghÄ©a vá»¥ ngÆ°á»i lao Ä‘á»™ng.
3. PhÃ¡t hiá»‡n vi pháº¡m Bá»™ luáº­t Lao Ä‘á»™ng 2019 (náº¿u cÃ³).
4. Gá»£i Ã½ Ä‘iá»u chá»‰nh há»£p lÃ½.
"""

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": "Báº¡n lÃ  chuyÃªn gia phÃ¡p lÃ½ Viá»‡t Nam, chuyÃªn vá» luáº­t lao Ä‘á»™ng."},
            {"role": "user", "content": prompt},
        ],
    }

    res = requests.post(LM_STUDIO_API, json=payload, timeout=180)
    result = res.json()
    answer = result["choices"][0]["message"]["content"]

    return {"summary": answer, "related_articles": related_laws}


# ğŸ’¬ CHATBOX PHÃP LÃ
class ChatRequest(BaseModel):
    message: str


@app.post("/chat")
async def chat_with_ai(req: ChatRequest):
    message = req.message.strip()
    if not message:
        return {"error": "Tin nháº¯n khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng."}

    related_laws = query_law(message, top_k=3)
    law_context = "\n\n".join(
    [f"Äiá»u {l['article_number']}: {l['article_title']}\n{l['content']}" for l in related_laws])


    prompt = f"""
Báº¡n lÃ  AI phÃ¡p lÃ½ Viá»‡t Nam, tÆ° váº¥n theo Bá»™ luáº­t Lao Ä‘á»™ng 2019.

--- CÃ¢u há»i ---
{message}

--- Äiá»u luáº­t liÃªn quan ---
{law_context}

YÃªu cáº§u:
- Tráº£ lá»i chÃ­nh xÃ¡c dá»±a trÃªn luáº­t.
- Náº¿u khÃ´ng cÃ³ Ä‘iá»u luáº­t phÃ¹ há»£p, hÃ£y nÃ³i "KhÃ´ng cÃ³ quy Ä‘á»‹nh cá»¥ thá»ƒ trong Bá»™ luáº­t Lao Ä‘á»™ng 2019".
- Giáº£i thÃ­ch ngáº¯n gá»n, dá»… hiá»ƒu, Ä‘Ãºng phÃ¡p lÃ½.
"""

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": "Báº¡n lÃ  chuyÃªn gia phÃ¡p lÃ½ Viá»‡t Nam, am hiá»ƒu Bá»™ luáº­t Lao Ä‘á»™ng 2019."},
            {"role": "user", "content": prompt},
        ],
    }

    try:
        res = requests.post(LM_STUDIO_API, json=payload, timeout=60)
        result = res.json()
        answer = result["choices"][0]["message"]["content"]
    except Exception as e:
        return {"error": f"Lá»—i khi gá»i LM Studio API: {e}"}

    return {
        "user_message": message,
        "ai_reply": answer,
        "related_articles": related_laws
    }
