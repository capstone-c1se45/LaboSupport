import os
import re
import json
import docx
import chromadb
from sentence_transformers import SentenceTransformer


DOCX_FILE = "219_2025_ND-CP_668418.docx" 

LAW_ID = "219/2025/Nƒê-CP" 
LAW_NAME = "Ngh·ªã ƒë·ªãnh 219/2025/Nƒê-CP v·ªÅ lao ƒë·ªông n∆∞·ªõc ngo√†i"
CHAPTER_DEFAULT = "Quy ƒë·ªãnh chung"

JSON_OUTPUT_PATH = "nghidinh219_chunk.json"
current_dir = os.path.dirname(os.path.abspath(__file__))
CHROMA_DB_PATH = os.path.join(current_dir, 'chroma_db') 
COLLECTION_NAME = "luatlaodong_chunks"

def parse_docx_to_sections(docx_path):
    """
    H√†m ƒë·ªçc file Word (D√πng logic c≈©, ho·∫°t ƒë·ªông t·ªët v·ªõi format ƒêi·ªÅu/Ch∆∞∆°ng)
    """
    if not os.path.exists(docx_path):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file {docx_path}")
        return []

    doc = docx.Document(docx_path)
    sections = []
    
    current_chapter = CHAPTER_DEFAULT
    current_section = None
    
    # Regex b·∫Øt d√≤ng "ƒêi·ªÅu X."
    article_pattern = re.compile(r'^ƒêi·ªÅu\s+(\d+)[\.\s]', re.IGNORECASE)
    # Regex b·∫Øt d√≤ng "Ch∆∞∆°ng I"
    chapter_pattern = re.compile(r'^Ch∆∞∆°ng\s+[IVX0-9]+', re.IGNORECASE)

    print(f"üìñ ƒêang ƒë·ªçc file: {docx_path}...")

    for para in doc.paragraphs:
        text = para.text.strip()
        if not text: continue 

        # B·∫Øt ch∆∞∆°ng
        if chapter_pattern.match(text):
            current_chapter = text
            if current_section:
                sections.append(current_section)
                current_section = None
            continue

        # B·∫Øt ƒëi·ªÅu
        match = article_pattern.match(text)
        if match:
            if current_section:
                sections.append(current_section)
            
            article_number = match.group(1)
            
            # T·∫°o ID Unique: 219-2025-ND-CP_dieu_1
            # ID n√†y KH√ÅC v·ªõi ID c·ªßa BLLƒê 2019 n√™n s·∫Ω kh√¥ng b·ªã ƒë√®
            clean_law_id = LAW_ID.replace("/", "-").replace(" ", "").replace("ƒê", "D") # Clean k·ªπ h∆°n
            section_id = f"{clean_law_id}_dieu_{article_number}"
            
            current_section = {
                "section_id": section_id,
                "law_id": LAW_ID,
                "law_name": LAW_NAME,
                "chapter": current_chapter,
                "law_reference": f"ƒêi·ªÅu {article_number} - {LAW_NAME}",
                "article_title": text,
                "chunk_index": 1,
                "content": text,
                "category": "Lao ƒë·ªông n∆∞·ªõc ngo√†i" # Category m·ªõi
            }
        else:
            if current_section:
                current_section["content"] += "\n" + text
    
    if current_section:
        sections.append(current_section)

    print(f"‚úÖ ƒê√£ t√°ch ƒë∆∞·ª£c {len(sections)} ƒëi·ªÅu lu·∫≠t t·ª´ Ngh·ªã ƒë·ªãnh m·ªõi.")
    return sections

def append_to_chromadb(sections):
    """
    H√†m th√™m d·ªØ li·ªáu v√†o ChromaDB (APPEND MODE)
    """
    print(f"üîÑ ƒêang k·∫øt n·ªëi ChromaDB t·∫°i: {CHROMA_DB_PATH}")
    
    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    
    # --- QUAN TR·ªåNG: D√πng get_or_create, KH√îNG delete ---
    collection = client.get_or_create_collection(name=COLLECTION_NAME)
    print(f"üìä S·ªë l∆∞·ª£ng vector hi·ªán t·∫°i tr∆∞·ªõc khi th√™m: {collection.count()}")

    embedding_func = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    
    ids = [s['section_id'] for s in sections]
    documents = [s['content'] for s in sections]
    metadatas = [{
        "law_id": s['law_id'],
        "section_id": s['section_id'],
        "law_name": s['law_name'],
        "article_title": s['article_title'],
        "law_reference": s['law_reference']
    } for s in sections]
    
    batch_size = 50
    total = len(ids)
    print(f"üì• ƒêang TH√äM {total} ƒëi·ªÅu lu·∫≠t m·ªõi v√†o DB...")
    
    for i in range(0, total, batch_size):
        end = min(i + batch_size, total)
        print(f"   - Processing batch {i}-{end}/{total}")
        
        # Nh·ªõ d√πng normalize_embeddings=True cho ƒë·ªìng b·ªô
        embeddings = embedding_func.encode(documents[i:end], normalize_embeddings=True).tolist()
        
        # --- QUAN TR·ªåNG: D√πng UPSERT thay v√¨ ADD ---
        # upsert: N·∫øu ID ch∆∞a c√≥ -> Th√™m m·ªõi. N·∫øu ID c√≥ r·ªìi -> C·∫≠p nh·∫≠t.
        collection.upsert(
            ids=ids[i:end],
            documents=documents[i:end],
            metadatas=metadatas[i:end],
            embeddings=embeddings
        )

    print(f"üéâ Ho√†n t·∫•t! T·ªïng s·ªë vector trong DB b√¢y gi·ªù: {collection.count()}")

def save_json_for_mysql(sections):
    with open(JSON_OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(sections, f, ensure_ascii=False, indent=2)
    print(f"üíæ ƒê√£ l∆∞u JSON cho MySQL: {JSON_OUTPUT_PATH}")

if __name__ == "__main__":
    # 1. Parse File m·ªõi
    sections = parse_docx_to_sections(DOCX_FILE)
    
    if sections:
        # 2. Append v√†o Chroma
        append_to_chromadb(sections)
        
        # 3. Xu·∫•t JSON
        save_json_for_mysql(sections)
        