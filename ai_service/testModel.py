from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_path = r"ai_service/model/Phi-3-mini-4k-instruct"  # Thay Ä‘Ãºng Ä‘Æ°á»ng dáº«n báº¡n clone

print("ğŸ”„ Äang load model...")
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float32,   # DÃ¹ng CPU thÃ¬ float32 á»•n Ä‘á»‹nh hÆ¡n
)

print("âœ… Model Ä‘Ã£ load xong!")

# Prompt test
prompt = "Viáº¿t má»™t Ä‘oáº¡n chÃ o buá»•i sÃ¡ng vui váº» báº±ng tiáº¿ng Viá»‡t."

# Tokenize vÃ  generate
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(
    **inputs,
    max_new_tokens=50,
    temperature=0.7,
)

# Giáº£i mÃ£ káº¿t quáº£
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("\nğŸ§  Káº¿t quáº£ model:")
print(response)
